<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SciLitLLM: How to Adapt LLMs for Scientific Literature Understanding"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="https://scilitllms.github.io"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>SciLitLLM: How to Adapt LLMs for Scientific Literature Understanding?</title>
  <link rel="icon" href="static/images/Microscope.png">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">SciLitLLM: How to Adapt LLMs for Scientific Literature Understanding?</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=1uFZs6EAAAAJ&hl=en">Sihang Li</a>*<sup style="color:#005587;">1</sup>,
              </span>
              <span class="author-block">
                <a href="https://jn-huang.github.io">Jin Huang</a>*<sup style="color:#FFD700;">3</sup>,
              </span>
              <span class="author-block">
                <a href="">Jiaxi Zhuang</a><sup style="color:#8C1515;">2</sup>,
              </span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=EWU3rdIAAAAJ&hl=en">Yaorui Shi</a><sup style="color:#005587;">1</sup>,
              </span>
              <span class="author-block">
                <a href="">Xiaochen Cai</a><sup style="color:#8C1515;">2</sup>,
              </span>
              <span class="author-block">
                <a href="">Mingjun Xu</a><sup style="color:#8C1515;">2</sup>,
              </span>
              <span class="author-block">
                <a href="https://xiangwang1223.github.io">Xiang Wang</a><sup style="color:#005587;">1</sup>,
              </span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=jk7qwmcAAAAJ&hl=zh-CN">Linfeng Zhang</a><sup style="color:#8C1515;">2</sup>,
              </span>
              <span class="author-block">
                <a href="https://guolinke.github.io">Guolin Ke</a><sup style="color:#8C1515;">2</sup>,
              </span>
              <span class="author-block">
                <a href="">Hengxing Cai</a><sup style="color:#8C1515;">2</sup>
              </span>
              <br>

              <div class="is-size-5 publication-authors">
                <span class="author-block"><sup style="color:#005587;">1</sup>University of Science and Technology of China,</span>
                <span class="author-block"><sup style="color:#8C1515">2</sup>DP Technology,</span>
                <span class="author-block"><sup style="color:#FFD700">3</sup>University of Michigan</span><br>
              
                <span class="paper-block"><b>* Equal Contribution</b></span><br>
                <!-- <span class="paper-block"><b>† Work done while interning at DP Technology</b></span><br> -->
                <span class="paper-block"><b></b></span><br>
                <span class="paper-block"><b>In submission. To appear at Foundation Models for Science Workshop, NeurIPS 2024</b></span>
              </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/abs/2408.15545" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="ai ai-arxiv"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/dptech-corp/Uni-SMART/tree/main/SciLitLLM" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- LLMs -->
                <span class="link-block">
                  <a href="https://huggingface.co/Uni-SMART/SciLitLLM-1.5" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <!-- <i class="fas fa-hugging-face"></i> -->
                    <i class="fa-solid fa-database"></i>
                  </span>
                  <span>Models</span>
                </a>
                
                <!-- Datasets -->
                <span class="link-block">
                  <a href="https://huggingface.co/datasets/Uni-SMART/SciLitIns" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <!-- <i class="fas fa-hugging-face"></i> -->
                    <i class="fa-solid fa-database"></i>
                  </span>
                  <span>Datasets</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>



<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">TL;DR</h2>
        <div class="content has-text-justified">
          <ul>
            <li>
              We devise <b>a pipeline to adapt general LLMs to scientific literature understanding</b>. It combines continual pre-training (CPT) and supervised fine-tuning (SFT) to enhance scientific knowledge base and instruction-following capabilities for specialized domain tasks.
            </li>
            <li>
              We propose <b>a novel domain instruction synthesis method</b> to curate instructions for scientific literature understanding, resulting in a new dataset - <b>SciLitIns</b>.
            </li>
            <li>
              <b>SciLitLLM</b>, trained through the proposed pipeline, outperforms leading open-source LLMs on scientific literature understanding.
            </li>
          </ul>
        </div>
      </div>
    </div>
  </div>
</section>



<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Performance Overview</h2>
        <div class="content has-text-justified">
          <div class="content has-text-centered">
            <img src="static/images/performance_plot.png" alt="Performance comparison" width="90%"/>
            <p> 
              Performance of <b>SciLitLLM</b> on scientific literature understanding benchmarks. SciLitLLM outperforms Llama3.1 and Qwen2.5 models with similar scales.
            </p>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Introduction</h2>
        <div class="content has-text-justified">
          <p>
            <b>Scientific literature understanding</b> involves the systematic evaluation and interpretation of scientific texts and publications, to identify trends and extract targeted information.
          </p>
          <p>
            Below is an example in <a href="https://arxiv.org/abs/2406.07835" target="_blank">SciRIFF</a>. The LLM is asked to understand the content of a biomedical research paper and then extract the targeted information. LLMs' potential might be hindered by two major barriers: 
            <ul>
              <li>
                Unfamiliarity with <b>scientific tasks</b>, leading to the inability of Galactica-6.7B to follow task instructions accurately.
              </li>
              <li>
                A lack of <b>scientific knowledge</b>, which results in errors such as the missing important entities in Llama-3.1-8B.
              </li>
            </ul>
          </p>          
          <div class="content has-text-centered">
            <img src="static/images/sci_lit_analysis_v3.png" alt="Problem Setup" class="interpolation-image"/>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Method</h2>
        <div class="content has-text-justified">
          <p>
            SciLitLLM utilizes a <b>two-stage pipeline</b> for enhancing domain-specific knowledge in LLMs.
          </p>
          <ul>
            <li>
              Stage 1, Continual Pre-Training (CPT): This step focuses on injecting domain-specific knowledge by pre-training on a curated scientific corpus. Over 73,000 textbooks and 625,000 research papers were processed and filtered to ensure high-quality data.
            </li>
            <li>
              Stage 2, Supervised Fine-Tuning (SFT): To fine-tune the model's instruction-following abilities, a novel set of scientific instructions called <b>SciLitIns</b> was created. This dataset encompasses diverse scientific tasks, guiding the model in understanding and executing complex scientific tasks.
            </li>
          </ul>
          <div class="content has-text-centered">
            <img src="static/images/SciLLM-Framework_v5.png" alt="SciLitLLM Framework"/>
          </div>
          <!-- <p>
            The combination of CPT and SFT enhances SciLitLLM's ability to understand and process specialized scientific literature effectively, resulting in a model that outperforms general-purpose LLMs in domain-specific benchmarks.
          </p> -->
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Stage 1: Scientific Knowledge Injection</h2>
        <div class="content has-text-justified">
          <p>
            High-quality scientific textbooks and research papers provide a wealth of scientific knowledge. However, we face some practical challenges:
          </p>
          <ul>
            <li>Formatting and grammar errors introduced during PDF parsing.</li>
            <li>Low-information segments, such as references and garbled text.</li>
          </ul>

          <div class="content has-text-centered">
            <img src="static/images/grammer_correction_eg.png" alt="Corpus Quality Assessment"/>
            <p> An example of formatting and grammer correction.</p>
          </div>

          <h3 class="title is-4">Formatting and Grammar Correction</h3>
          <p>
            To address these challenges, we implemented the following modules:
          </p>
          <ul>
            <li><b>Formatting and Grammar Correction</b>: We utilized the Llama3-8B model to correct errors introduced during the PDF parsing process.</li>
            <li><b>CPT Quality Filtering</b>: Llama3-70B was used to score a subset of the texts. These scores were then used as labels to train a lightweight classifier for evaluating the quality of the training corpus.</li>
          </ul>
          <div class="content has-text-centered">
            <img src="static/images/corpus_example_v3.png" alt="Corpus Quality Assessment" width="80%"/>
            <p> Examples of high and low-quality CPT text and SFT instructions.</p>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Stage 2: Scientific Instruction Fine-Tuning</h2>
        <div class="content has-text-justified">
          <p>
            To address the scarcity of scientific instructions and the high costs associated with manual annotation, we developed a novel instruction generation and quality control process. 
            This involved creating <b>a probability table of domain-specific keywords</b> and <b>a list of scientific task descriptions</b>. We then sampled keywords and tasks to generate a diverse dataset of domain-specific instructions by GPT-4o.
            <div class="content has-text-centered">
              <img src="static/images/scilitins_example.png" alt="Corpus Quality Assessment"/>
              <p> An Example in SciLitIns.</p>
            </div>
            To ensure the quality of the generated instructions, we introduced an Instruction Quality Control module:
          </p>
          <ul>
            <li><b>Heuristic Deduplication</b>: We calculated the Levenshtein distance to filter out the top 20% of highly repetitive instructions, ensuring the diversity of the instruction set.</li>
            <li><b>Model-Based Filtering</b>: We used Llama3-70B-Instruct to evaluate the generated instructions across five dimensions: clarity, complexity, correctness, usefulness, and adaptability. Instructions with an average score below 4 were excluded to maintain the quality of the instruction set.</li>
          </ul>
          <div class="content has-text-centered">
            <img src="static/images/sft-scores.png" alt="Instruction Quality Control" width="80%"/>
            <p> The quality assessment of SciLitIns.</p>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Evaluation Results</h2>
        <div class="content has-text-justified">
          <div class="content has-text-centered">
            <img src="static/images/evaluation.png" alt="Performance comparison"/>
            <p> 
              Model performances on scientific literature understanding benchmarks: <a href="https://arxiv.org/abs/2403.01976" target="_blank">SciAssess</a> and <a href="https://arxiv.org/abs/2406.07835" target="_blank">SciRIFF</a>.
            </p>
          </div>
          <p>
            <b>Key Observations:</b>
            <ul>
              
              <li>SciLitLLM-7B-Instruct achieves the highest performance in all 4 domains on SciAssess, outperforming the second-best model by 4.0%. On SciRIFF, it surpasses baseline models by a substantial margin of 10.1%</li>
              <li>SciLitLLM-14B-Instruct shows a 4.6% and 7.5% performance improvement over Qwen2-14B-Instruct on SciAssess and SciRIFF, respectively. <b>It even outperforms other open-source models with five times more parameters.</b> </li>
            </ul>
          </p>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Ablation Study</h2>
        <div class="content has-text-justified">
          <p>
            To assess the contribution of each component in the SciLitLLM pipeline, we conducted a comprehensive ablation study.
          </p>
          <ul>
            <li>
              <b>Effect of Continual Pre-Training</b>: Removing the CPT stage resulted in a significant drop in domain-specific performance. The results shows that the CPT stage is essential to improve scientific literature understanding ability.
              <div class="content has-text-centered">
                <img src="static/images/ablation1.png" alt="Effect of CPT Removal" width="65%"/>
                <!-- <p>Impact of removing the CPT stage on performance.</p> -->
              </div>
            </li>
            <li>
              <b>SFT on SciLitIns</b>: Our synthetic dataset SciLitIns effectively improves performance on both benchmarks.
              <div class="content has-text-centered">
                <img src="static/images/ablation2.png" alt="Impact of Instruction Dataset Quality" width="65%"/>
                <!-- <p>Ablation study of SFT data recipes.</p> -->
              </div>
            </li>
            <li>
              <b>Influence of Instruction Filter</b>: Applying the filter improves the performance of SciLitLLM-7B on SciAssess (+0.8%) and SciRIFF (+0.5%). This shows that our proposed filter refines the quality of SciLitIns .
              <div class="content has-text-centered">
                <img src="static/images/ablation3.png" alt="Influence of Data Volume" width="60%"/>
                <!-- <p>Ablation study of SFT instruction quality filtering.</p> -->
              </div>
            </li>
          </ul>
        </div>
      </div>
    </div>
  </div>
</section>




<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <p>Please consider citing our paper if you find our work helpful!</p>
      <h2 class="title">BibTeX</h2>
      <pre><code>@misc{li2024scilitllmadaptllmsscientific,
    title={SciLitLLM: How to Adapt LLMs for Scientific Literature Understanding}, 
    author={Sihang Li and Jin Huang and Jiaxi Zhuang and Yaorui Shi and Xiaochen Cai and Mingjun Xu and Xiang Wang and Linfeng Zhang and Guolin Ke and Hengxing Cai},
    year={2024},
    eprint={2408.15545},
    archivePrefix={arXiv},
    primaryClass={cs.LG},
    url={https://arxiv.org/abs/2408.15545}, 
}</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- Default Statcounter code for SciLitLLMs https://scilitllms.github.io -->
<script type="text/javascript">
  var sc_project=13049407; 
  var sc_invisible=1; 
  var sc_security="6f2045b1"; 
  </script>
  <script type="text/javascript"
  src="https://www.statcounter.com/counter/counter.js" async></script>
  <noscript><div class="statcounter"><a title="Web Analytics"
  href="https://statcounter.com/" target="_blank"><img class="statcounter"
  src="https://c.statcounter.com/13049407/0/6f2045b1/1/" alt="Web Analytics"
  referrerPolicy="no-referrer-when-downgrade"></a></div></noscript>

  <!-- End of Statcounter Code -->

  </body>
  </html>
